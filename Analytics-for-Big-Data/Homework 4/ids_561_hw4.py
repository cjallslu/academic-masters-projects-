# -*- coding: utf-8 -*-
"""IDS 561 - HW4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQ8BOuVO0Us2yGk5zWNQwYFxVRoN5sDr

# **IDS 561: Homework 4**
Son Nguyen

Sean O'Reilly

CJ All

# INSTALL DEPENDENCIES
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

"""# GET SPARK INSTALLER

"""

!wget -q http://apache.mirrors.pair.com/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz

!ls

!tar -xvf spark-2.4.7-bin-hadoop2.7.tgz

!ls

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark import SparkContext
sc =SparkContext.getOrCreate()

"""# READ THE DATASET"""

from urllib.request import urlretrieve
import pandas as pd

iris = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'

urlretrieve(iris)

df = pd.read_csv(iris, sep=',', header=None)

df

from io import StringIO

df = spark.read.format('com.databricks.spark.csv').\
                       options(header='true', \
                       inferschema='true').\
            load("/content/drive/MyDrive/IDS 561/hw4/iris.csv",header=True);

df.show(5,True)

df.describe().show()

df.columns

df2 = df.withColumnRenamed("sepal.length","sepallength")\
.withColumnRenamed("sepal.width","sepalwidth")\
.withColumnRenamed("petal.length","petallength")\
.withColumnRenamed("petal.width","petalwidth")

from pyspark.ml.feature import VectorAssembler

features = ('sepallength', 'sepalwidth', 'petallength', 'petalwidth')

from pyspark.sql.types import StructType, StructField, NumericType

assembler = VectorAssembler(inputCols=features,outputCol="features")

dataset=assembler.transform(df2)

dataset.select("features").show(truncate=False)

from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.clustering import KMeans

# Trains a k-means model.
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(dataset)

# Make predictions
predictions = model.transform(dataset)

# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))


# Evaluate clustering.
cost = model.computeCost(dataset)
print("Within Set Sum of Squared Errors = " + str(cost))

# Shows the result.
print("Cluster Centers: ")
ctr=[]
centers = model.clusterCenters()
for center in centers:
    ctr.append(center)
    print(center)

import numpy as np
cost = np.zeros(20)
sil = np.zeros(20)
for k in range(2,20):
    kmeans = KMeans()\
            .setK(k)\
            .setSeed(1)

    model = kmeans.fit(dataset)
    predictions = model.transform(dataset)
    evaluator = ClusteringEvaluator()
    sil[k] = evaluator.evaluate(predictions)
    cost[k] = model.computeCost(dataset) # requires Spark 2.0 or later

import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import seaborn as sbs
from matplotlib.ticker import MaxNLocator

fig, ax = plt.subplots(1,1, figsize =(8,6))
ax.plot(range(2,20),cost[2:20])
ax.set_xlabel('k')
ax.set_ylabel('cost')
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
plt.show()

sil

"""â€œk=6 gives the best performance, Silhouette =0.54792778 "

Reference
https://runawayhorse001.github.io/LearningApacheSpark/clustering.html
https://spark.apache.org/docs/latest/ml-clustering.html
https://spark.apache.org/docs/2.0.1/ml-classification-regression.html
https://www.bmc.com/blogs/python-spark-k-means-example/
"""

